import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

import matplotlib.pyplot as plt
import seaborn as sns

import os
import sys
from datetime import datetime
import logging
import io
import time

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = f"outputs/autoencoding_kmeans_{timestamp}"
# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å°±å»ºç«‹
if not os.path.exists(output_dir):
    os.makedirs(output_dir, exist_ok=True)
    print(f"å·²å»ºç«‹è³‡æ–™å¤¾ï¼š{output_dir}")
else:
    print(f"è³‡æ–™å¤¾å·²å­˜åœ¨ï¼š{output_dir}")

# å»ºç«‹ timestamp log æª”æ¡ˆ
log_path = os.path.join(output_dir, f"run_{timestamp}.log")

# è¨­å®š logging
logging.basicConfig(
    level=logging.INFO,  # å¯æ”¹ DEBUG / WARNING / ERROR
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler()  # åŒæ™‚è¼¸å‡ºåˆ° console
    ]
)
logger = logging.getLogger(__name__)
# ====== è¨˜éŒ„æ•´é«”é–‹å§‹æ™‚é–“ ======
experiment_start = time.time()
logger.info("===== Experiment started =====")
logger.info(f"Logging started. Output file: {log_path}")

class Timer:
    def __init__(self, logger):
        self.logger = logger
        self.start = time.time()

    def log(self, stage_name):
        now = time.time()
        elapsed = now - self.start
        self.start = now  # é‡ç½®èµ·é»

        hours = int(elapsed // 3600)
        minutes = int((elapsed % 3600) // 60)
        seconds = elapsed % 60

        self.logger.info(
            f"{stage_name} finished in {hours}h {minutes}m {seconds:.2f}s"
        )

# å»ºç«‹ Timer 
timer = Timer(logger)
# é™ä½ TensorFlow çš„ log ç­‰ç´šï¼ˆé¿å…å¤ªå¤šè³‡è¨Šå¹²æ“¾ï¼‰
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
logger.info("Num GPUs Available: %d", len(tf.config.list_physical_devices('GPU')))

# è®€å– CSV æ–‡ä»¶ï¼Œå°‡æ•¸æ“šåŠ è¼‰åˆ° DataFrame ä¸­
df = pd.read_csv("./datasets/dataset.csv")
buffer = io.StringIO()
df.info(buf=buffer)
info_str = buffer.getvalue()
logger.info("DataFrame info:\n%s", info_str)

# -----------------------------
# æª¢æŸ¥ç¼ºå¤±å€¼
# -----------------------------
missing_cols = df.isna().sum()
missing_cols = missing_cols[missing_cols > 0]

for col, missing in missing_cols.items():
    logger.info(f"{col}: {missing} missing")

'''
å®šç¾©éŸ³è¨Šç‰¹å¾µèˆ‡ metadata ç‰¹å¾µ
audio_featuresï¼šSpotify çš„ 9 å€‹é€£çºŒå‹éŸ³è¨Šç‰¹å¾µ
meta_featuresï¼šæ­Œæ›²çš„æ­Œæ‰‹ã€æµæ´¾ã€ç†±é–€åº¦
ç§»é™¤é‡è¤‡æ­Œæ›²
é¿å…åŒåæ­Œæ›²é€ æˆæ¨¡å‹åå·®ã€‚
å°‡é¡åˆ¥ç‰¹å¾µè½‰æ›ç‚ºæ•¸å€¼ï¼ˆLabel Encodingï¼‰
Autoencoder èˆ‡ KMeans ç„¡æ³•è™•ç†å­—ä¸²ï¼Œå› æ­¤ï¼š

artists â†’ artists_le
track_genre â†’ genre_le
å»ºç«‹æœ€çµ‚ç‰¹å¾µé›†åˆ all_features
åŒ…å«ï¼š

9 å€‹ audio features
popularityï¼ˆæ•¸å€¼ï¼‰
artists_leï¼ˆç·¨ç¢¼å¾Œçš„æ­Œæ‰‹ï¼‰
genre_leï¼ˆç·¨ç¢¼å¾Œçš„æ›²é¢¨
'''
audio_features = [
    'danceability', 'energy', 'valence', 'liveness', 'acousticness',
    'instrumentalness', 'speechiness', 'tempo', 'loudness'
    ]
meta_features = ['artists','popularity','track_genre']

# å»ºç«‹å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹ df
df = df.copy()

# -----------------------------
# æª¢æŸ¥ç¼ºå¤±å€¼
# -----------------------------
missing_cols = df.isna().sum()
missing_cols = missing_cols[missing_cols > 0]

for col, missing in missing_cols.items():
    print(f"{col}: {missing} missing")

# è£œç¼ºå¤±å€¼ï¼ˆé¿å… NaN é€ æˆ drop_duplicates è¡Œç‚ºä¸ä¸€è‡´ï¼‰
logger.info("Filling missing values for 'track_name' and 'artists'")
df['track_name'] = df['track_name'].fillna("Unknown Track")
df['artists'] = df['artists'].fillna("Unknown Artist")

# ä¾ track_name & artists åˆªé™¤é‡è¤‡æ­Œæ›²ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç¾çš„è¨˜éŒ„ï¼Œé¿å…æ¨è–¦ç³»çµ±æ¨è–¦é‡è¤‡æ­Œæ›²
logger.info("Dropping duplicate songs based on 'track_name' and 'artists'")
df = df.drop_duplicates(subset=['track_name','artists'], keep='first').reset_index(drop=True)

'''
åŒæ™‚å°å‡ºç¸½ç­†æ•¸èˆ‡ unique æ•¸é‡ å°å¾ŒçºŒåš Label Encodingã€Embeddingã€Autoencoder éƒ½å¾ˆæœ‰å¹«åŠ©ï¼Œå› ç‚ºå¯ä»¥å¿«é€ŸçŸ¥é“ï¼š
- artists æ˜¯å¦ç‚º high-cardinality ç‰¹å¾µ
- æ˜¯å¦éœ€è¦é¿å… One-Hot
- æ˜¯å¦éœ€è¦ embedding layer æˆ– autoencoder
'''
unique_artists = df['artists'].unique().tolist()
logger.info("Artists: count=%d", len(unique_artists))
unique_genres = df['track_genre'].unique()
logger.info("Genres: count=%d, name=%s", len(unique_genres), unique_genres)

'''
Label Encoding çš„ä½¿ç”¨æ™‚æ©Ÿ
1. é¡åˆ¥æœ¬èº«æœ‰é †åºï¼ˆordinalï¼‰
2. é¡åˆ¥æ•¸é‡æ¥µå¤§ï¼ˆhigh cardinalityï¼‰
'''
logger.info("Label encoding 'artists'")
# Label encode artists
le_artist = LabelEncoder()
df['artists_le'] = le_artist.fit_transform(df['artists'])

# # Label encode track_genre
# le_genre = LabelEncoder()
# df['genre_le'] = le_genre.fit_transform(df['track_genre'])

'''
One-Hot Encoding çš„ä½¿ç”¨æ™‚æ©Ÿ
1. é¡åˆ¥æ²’æœ‰é †åºï¼ˆnominalï¼‰One-Hot èƒ½è®“ Autoencoder è‡ªå·±å­¸åˆ°é¡åˆ¥ä¹‹é–“çš„èªæ„è·é›¢ï¼Œè€Œä¸æœƒè¢«å‡é †åºå¹²æ“¾ã€‚
2. é¡åˆ¥æ•¸é‡ä¸å¤§ï¼ˆ< 1000ï¼‰One-Hot ç¶­åº¦ä¸æœƒå¤ªå¤§ï¼ŒAutoencoder å¯ä»¥æœ‰æ•ˆå£“ç¸®ã€‚
3. ä½ å¸Œæœ› Autoencoder å­¸åˆ°ã€Œé¡åˆ¥ä¹‹é–“çš„ç›¸ä¼¼æ€§ã€
'''

'''
åŒæ™‚å° artists èˆ‡ genre åš one-hot encoding æœƒå°è‡´ç¶­åº¦éé«˜ï¼Œè¨˜æ†¶é«”çˆ†ç‚¸
'''
logger.info("One-Hot encoding 'track_genre'")
# ===== One-Hot Encoding for genre =====
ohe = OneHotEncoder(sparse_output=False)

# åš One-Hot
ohe_features = ohe.fit_transform(df[['track_genre']])

# ===== PCA å£“ç¸®æˆ embeddingï¼ˆä½ å¯èª¿æ•´ç¶­åº¦ï¼‰=====
pca = PCA(n_components=5, random_state=42)
emb = pca.fit_transform(ohe_features)

# ===== åŠ å› DataFrame =====
emb_cols = [f"genre_emb_{i}" for i in range(5)]
df[emb_cols] = emb

# ===== ç”¢ç”Ÿ genre_leï¼ˆä¿ç•™çµ¦å¾ŒçºŒç¨‹å¼ç¢¼ä½¿ç”¨ï¼‰=====
# ç”¨ embedding çš„ç¬¬ä¸€ç¶­ç•¶ä½œ genre_leï¼ˆå¯æ’åºã€æœ‰èªæ„ï¼‰
df['genre_le'] = df['genre_emb_0']

# æœ€çµ‚ç‰¹å¾µé›†åˆ
all_features = audio_features + ['popularity', 'artists_le', 'genre_le']


'''
Scaling + PCAï¼ˆå…ˆçœ‹è³‡æ–™çµæ§‹ï¼‰
é€™è£¡çš„ PCA å…ˆç•¶ä½œã€Œè³‡æ–™æ¢ç´¢ç”¨ã€ï¼Œç”¨ä¾†ç¢ºèªéŸ³æ¨‚ç‰¹å¾µæ˜¯å¦æœ‰å¯åˆ†ç¾¤çš„çµæ§‹
(1) è³‡æ–™æ˜¯å¦å‘ˆç¾å‡ºè‡ªç„¶çš„ç¾¤é›†ï¼Ÿ ä¾‹å¦‚ï¼š

æœ‰æ˜é¡¯çš„ blob å½¢ç‹€å¤§è‡´åœ“æˆ–æ©¢åœ“çš„è³‡æ–™é»ç¾¤ â†’ KMeans å¯èƒ½å¾ˆå¥½ç”¨
å‘ˆç¾é•·æ¢ç‹€ / æœˆç‰™å½¢ â†’ KMeans ä¸é©åˆï¼ŒHDBSCAN / UMAP æœƒæ›´å¥½
å®Œå…¨ä¸€åœ˜ â†’ å¯èƒ½éœ€è¦éç·šæ€§ embeddingï¼ˆAutoencoder / UMAPï¼‰
(2) æ˜¯å¦æœ‰é›¢ç¾¤é»ï¼Ÿ

è‹¥æœ‰æ˜é¡¯å­¤ç«‹é» â†’ clustering æœƒå—å½±éŸ¿
å¯èƒ½éœ€è¦å…ˆåš outlier removal
(3) æ˜¯å¦æœ‰ç·šæ€§å¯åˆ†æ€§ï¼Ÿ

è‹¥è³‡æ–™å‘ˆç¾ç·šæ€§æ–¹å‘ â†’ PCA / KMeans æœƒè¡¨ç¾ä¸éŒ¯
è‹¥è³‡æ–™å‘ˆç¾å½æ›²ã€éç·šæ€§ â†’ Autoencoder embedding æœƒæ›´é©åˆ
(4) æ˜¯å¦æœ‰ã€Œç¶­åº¦å¡Œç¸®ã€å•é¡Œï¼Ÿ å¦‚æœ 2D PCA çœ‹èµ·ä¾†åƒä¸€æ¢ç·šï¼š

ä»£è¡¨è³‡æ–™é«˜åº¦ç›¸é—œ
clustering å¯èƒ½æœƒä¸ç©©å®š
Autoencoder å¯èƒ½éœ€è¦æ›´å°çš„ bottleneck
'''
X = df[all_features].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA ç”¨æ–¼æª¢æŸ¥/å¯è¦–åŒ–ï¼ˆå…ˆå£“åˆ°èƒ½è§£é‡‹ 95% varianceï¼‰
pca_full = PCA(n_components=0.95, random_state=42)
Xp = pca_full.fit_transform(X_scaled)
logger.info("PCA shape: %s", Xp.shape)

# å†å£“åˆ° 2 ç¶­åšè¦–è¦ºåŒ–ç”¨
pca_2d = PCA(n_components=2, random_state=42)
Xp_2d = pca_2d.fit_transform(X_scaled)

plt.figure(figsize=(7,6))
plt.title("PCA 2D (no clustering yet)")
sns.scatterplot(
    x=Xp_2d[:, 0], y=Xp_2d[:, 1], s=8, color='gray' #ç°è‰²é»ä¸æœƒå¹²æ“¾è¦–è¦º
)
plt.xlabel("PC1")
plt.ylabel("PC2")
# plt.show()
plt.savefig(f"./{output_dir}/pca_2d_no_clustering.png", dpi=300, bbox_inches='tight')
plt.close()

# ==================================
# ====== å»ºç«‹ Autoencoder æ¨¡å‹ ======
# input_dim = X_scaled.shape[1]
# encoding_dim = 16

# input_layer = layers.Input(shape=(input_dim,))

# # Encoder
# x = layers.Dense(64, activation='relu')(input_layer)
# x = layers.Dense(32, activation='relu')(x)
# bottleneck = layers.Dense(encoding_dim, activation='linear', name='bottleneck')(x)

# # Decoder
# x = layers.Dense(32, activation='relu')(bottleneck)
# x = layers.Dense(64, activation='relu')(x)
# output_layer = layers.Dense(input_dim, activation='linear')(x)

# autoencoder = models.Model(inputs=input_layer, outputs=output_layer)
# encoder = models.Model(inputs=input_layer, outputs=bottleneck)

# autoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')
# autoencoder.summary()
# ==================================

# ==================================
# ====== å»ºç«‹ Autoencoder æ¨¡å‹ åŠ å…¥ L2 æ­£å‰‡åŒ– + Dropout ======
input_dim = X_scaled.shape[1]
encoding_dim = 16

# ===== Autoencoder çµæ§‹ =====
input_layer = layers.Input(shape=(input_dim,))

# ===== Encoder =====
x = layers.Dense(
    64, activation='relu',
    kernel_regularizer=l2(1e-4)
)(input_layer)
x = layers.Dropout(0.2)(x)

x = layers.Dense(
    32, activation='relu',
    kernel_regularizer=l2(1e-4)
)(x)
x = layers.Dropout(0.2)(x)

bottleneck = layers.Dense(
    encoding_dim, activation='linear',
    name='bottleneck'
)(x)

# ===== Decoder =====
x = layers.Dense(
    32, activation='relu',
    kernel_regularizer=l2(5e-5)
)(bottleneck)
# Decoder dropout é™ä½
x = layers.Dropout(0.1)(x)

x = layers.Dense(
    64, activation='relu',
    kernel_regularizer=l2(5e-5)
)(x)
# æœ€å¾Œä¸€å±¤é€šå¸¸ä¸åŠ  Dropout

output_layer = layers.Dense(input_dim, activation='linear')(x)

# ===== Models =====
autoencoder = models.Model(inputs=input_layer, outputs=output_layer)
encoder = models.Model(inputs=input_layer, outputs=bottleneck)

autoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')

autoencoder.summary()
# ==================================

# è¨“ç·´ Autoencoder
logger.info("\n===== Training Autoencoder =====")
history = autoencoder.fit(
    X_scaled, X_scaled,
    epochs=200,
    batch_size=256,
    shuffle=True,
    validation_split=0.1,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
    ],
    verbose=0
)
# è¦–è¦ºåŒ–è¨“ç·´éç¨‹
plt.figure(figsize=(10, 5))

plt.plot(history.history['loss'], label='Training Loss', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)

plt.title("Autoencoder Training Curve", fontsize=16)
plt.xlabel("Epoch", fontsize=14)
plt.ylabel("MSE Loss", fontsize=14)
plt.legend(fontsize=12)
plt.grid(alpha=0.3)
# plt.show()
plt.savefig(f"./{output_dir}/autoencoder_training_curve.png", dpi=300, bbox_inches='tight')
plt.close()
timer.log("Autoencoder training")

# å–å‡º Autoencoder embedding
X_emb = encoder.predict(X_scaled)
logger.info("Embedding shape: %s", X_emb.shape)

# ====== PCA 2D for visualization ======
pca_emb_2d = PCA(n_components=2, random_state=42)
X_emb_2d = pca_emb_2d.fit_transform(X_emb)



# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# è¼¸å…¥æƒ³æ¯”è¼ƒçš„ k å€¼
k_list = [8, 10, 12,14,16]

# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!



logger.info("\n===== åˆ†ç¾¤å“è³ªæŒ‡æ¨™æ¯”è¼ƒ =====")

# æ¯å€‹ k è¼¸å‡ºä¸€å¼µç¨ç«‹åœ–ç‰‡
for k in k_list:

    # ===== KMeans åˆ†ç¾¤ ======
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels_np = kmeans.fit_predict(X_emb)

    # ===== è¨ˆç®—åˆ†ç¾¤å“è³ªæŒ‡æ¨™ ======
    sil = silhouette_score(X_emb, labels_np)
    ch = calinski_harabasz_score(X_emb, labels_np)
    dbi = davies_bouldin_score(X_emb, labels_np)

    logger.info("\n--- k = %d ---", k)
    logger.info("Silhouette Score: %f", sil)
    logger.info("Calinski-Harabasz Score: %f", ch)
    logger.info("Davies-Bouldin Index: %f", dbi)

    # ===== æ¯å€‹ k å»ºç«‹ä¸€å¼µæ–°çš„åœ– =====
    plt.figure(figsize=(7, 6))

    sns.scatterplot(
        x=X_emb_2d[:, 0],
        y=X_emb_2d[:, 1],
        hue=labels_np,
        palette="Paired",
        s=10,
        linewidth=0
    )

    plt.title(f"KMeans Clustering (k={k})")
    plt.xlabel("PC1")
    plt.ylabel("PC2")

    # é¡¯ç¤º legend
    plt.legend(title=f"k={k}", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()

    # ===== è¼¸å‡ºç¨ç«‹åœ–ç‰‡ =====
    plt.savefig(f"{output_dir}/kmeans_clustering_k{k}.png",
                dpi=300, bbox_inches='tight')

    plt.close()

# è¼¸å‡ºæ¯å€‹ k çš„ KMeans labels
for k in k_list:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels_np = kmeans.fit_predict(X_emb)

    # å»ºç«‹æ¬„ä½åç¨±ï¼Œä¾‹å¦‚ cluster_ae_kmeans_k8
    col_name = f"cluster_ae_kmeans_k{k}"
    df[col_name] = labels_np

    print(f"å·²å¥—ç”¨ k = {k} çš„åˆ†ç¾¤çµæœï¼Œæ¬„ä½åç¨±ï¼š{col_name}")

# æ¯å€‹ k å»ºç«‹ä¸€å¼µæ–°çš„åœ–
for k in k_list:
    cluster_col = f"cluster_ae_kmeans_k{k}"   # å°æ‡‰æ¬„ä½åç¨±
    plt.figure(figsize=(7, 6))

    sns.scatterplot(
        x=Xp_2d[:, 0],
        y=Xp_2d[:, 1],
        hue=df[cluster_col],
        palette="Paired",
        s=10,
        linewidth=0
    )

    plt.title(f"Original PCA 2D (k={k})")
    plt.xlabel("Original PC1")
    plt.ylabel("Original PC2")

    # é¡¯ç¤º legendï¼ˆå¦‚æœä½ æƒ³é—œæ‰å¯æ”¹æˆç©º legendï¼‰
    plt.legend(title=f"k={k}", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()
    # plt.show()
    # æ¯å€‹ k è¼¸å‡ºä¸€å¼µç¨ç«‹åœ–ç‰‡
    plt.savefig(f"{output_dir}/original_pca_2d_k{k}.png",
                dpi=300, bbox_inches='tight')

    plt.close()

timer.log("KMeans clustering")


"""
è‡ªå‹•å‘½åæ¯å€‹ clusterï¼Œä¿è­‰åç¨±å”¯ä¸€ä¸”å¯è®€
"""
def auto_name_clusters_by_k(df, cluster_col='cluster_ae_kmeans', semantic_features=None,name_col="cluster_name"):
    if semantic_features is None:
        semantic_features = [
            'energy', 'danceability', 'valence', 'acousticness',
            'instrumentalness', 'tempo', 'loudness',
            'speechiness', 'liveness'
        ]

    # è¨ˆç®—æ¯å€‹ cluster çš„ä¸­å¿ƒé»
    centroids = df.groupby(cluster_col)[semantic_features].mean()

    # å»ºç«‹ quantile å€é–“ï¼ˆæ¯å€‹ feature æœƒæœ‰ä½ã€ä¸­ã€é«˜ä¸‰æ®µï¼‰
    quantiles = df[semantic_features].quantile([0.33, 0.66])
    q33 = quantiles.loc[0.33]
    q66 = quantiles.loc[0.66]

    cluster_names = {}
    used_names = set()

    for cluster_id, row in centroids.iterrows():
        descriptors = []

        for feature in semantic_features:
            # å¤§å¯«é–‹é ­ç‰¹å¾µåç¨±ï¼ˆenergy â†’ Energyï¼‰
            feat_name = feature.capitalize()

            # å–è©²ç‰¹å¾µçš„ quantile åˆ‡é»
            f33 = q33[feature]
            f66 = q66[feature]

            # ä½
            if row[feature] < f33:
                descriptors.append(f"Low {feat_name}")
            # é«˜
            elif row[feature] > f66:
                descriptors.append(f"High {feat_name}")
            # ä¸­é–“å‰‡ä¸å‘½åï¼ˆé¿å…å¤ªå†—é•·ï¼‰
            else:
                continue

        # è‹¥å…¨éƒ¨éƒ½æ˜¯ä¸­é–“æ•¸å€¼ â†’ çµ¦é è¨­æè¿°
        if not descriptors:
            descriptors.append("Balanced")

        # åˆä½µåç¨±
        name = " / ".join(descriptors)

        # é¿å…åç¨±é‡è¤‡
        if name in used_names:
            name = f"{name} (Cluster {cluster_id})"
        used_names.add(name)

        cluster_names[cluster_id] = name

    # df['cluster_name'] = df[cluster_col].map(cluster_names)
    # df[name_col] = cluster_names[df[cluster_col]]
    df[name_col] = df[cluster_col].map(cluster_names)


    return df, cluster_names

# è¦–è¦ºåŒ–è‡ªå‹•å‘½åå¾Œçš„çµæœ
semantic_features = [
            'energy', 'danceability', 'valence', 'acousticness',
            'instrumentalness', 'tempo', 'loudness',
            'speechiness', 'liveness'
        ]

for k in k_list:

    cluster_col = f"cluster_ae_kmeans_k{k}"
    name_col = f"cluster_name_k{k}"

    # è‡ªå‹•å‘½å cluster
    df, cluster_names = auto_name_clusters_by_k(
        df,
        cluster_col=cluster_col,
        semantic_features=semantic_features,
        name_col=name_col
    )

    # æ¯å€‹ k å»ºç«‹ä¸€å¼µæ–°çš„åœ–
    plt.figure(figsize=(7, 6))

    sns.scatterplot(
        x=X_emb_2d[:, 0],
        y=X_emb_2d[:, 1],
        hue=df[name_col],
        palette="Paired",
        s=10,
        linewidth=0
    )

    plt.title(f"KMeans on AE Embedding (k={k})")
    plt.xlabel("Embedding PC1")
    plt.ylabel("Embedding PC2")

    # é¡¯ç¤º cluster åç¨±
    plt.legend(title=f"k={k}", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()

    # æ¯å€‹ k è¼¸å‡ºä¸€å¼µç¨ç«‹åœ–ç‰‡
    plt.savefig(f"{output_dir}/kmeans_on_ae_embedding_k{k}.png",
                dpi=300, bbox_inches='tight')

    plt.close()

def recommend_song_ae_cluster(cluster_col,embedding_cols,song_name,artist=None,n_recommendations=5,max_per_cluster=2):
    """
    Autoencoder + Cluster + Cosine Similarity æ¨è–¦ç³»çµ±
    - cluster_col: ä½¿ç”¨å“ªå€‹ cluster æ¬„ä½ï¼ˆä¾‹å¦‚ 'cluster_ae_kmeans'ï¼‰
    - embedding_cols: Autoencoder embedding æ¬„ä½ï¼ˆlistï¼‰
    - max_per_cluster: è·¨ cluster æ™‚ï¼Œæ¯å€‹ cluster æœ€å¤šå–å¹¾é¦–
    """

    # æ‰¾ç›®æ¨™æ­Œæ›²
    if artist:
        song = df[(df['track_name'].str.lower() == song_name.lower()) &
                  (df['artists'].str.lower() == artist.lower())]
    else:
        song = df[df['track_name'].str.lower() == song_name.lower()]

    if song.empty:
        logger.info("âŒ Song '%s' not found in dataset.", song_name)
        return None

    song_index = song.index[0]
    song_emb = df.loc[song_index, embedding_cols].values.reshape(1, -1)
    song_cluster = df.loc[song_index, cluster_col]

    # åŒ cluster ç¯©é¸
    same_cluster = df[(df[cluster_col] == song_cluster) & (df.index != song_index)].copy()

    # è¨ˆç®— cosine similarity
    same_cluster['distance'] = cosine_similarity(
        same_cluster[embedding_cols].values, song_emb
    ).reshape(-1)

    # å…ˆå–åŒ cluster çš„æ¨è–¦
    recommendations = same_cluster.sort_values('distance', ascending=False).head(n_recommendations)

    # fallbackï¼šè·¨ clusterï¼Œä½†é™åˆ¶æ¯å€‹ cluster æœ€å¤š max_per_cluster é¦–
    if len(recommendations) < n_recommendations:

        remaining_n = n_recommendations - len(recommendations)

        other = df[df.index != song_index].copy()
        other = other[~other.index.isin(recommendations.index)]

        # è¨ˆç®— cosine similarity
        other['distance'] = cosine_similarity(
            other[embedding_cols].values, song_emb
        ).reshape(-1)

        # æŒ‰ cluster åˆ†çµ„ï¼Œæ¯å€‹ cluster å– max_per_cluster é¦–
        fallback_list = []
        for c, group in other.groupby(cluster_col):
            top_c = group.sort_values('distance', ascending=False).head(max_per_cluster)
            fallback_list.append(top_c)

        fallback_df = pd.concat(fallback_list).sort_values('distance', ascending=False)

        # å–å‰©ä¸‹éœ€è¦çš„æ•¸é‡
        fallback_final = fallback_df.head(remaining_n)

        # åˆä½µ
        recommendations = pd.concat([recommendations, fallback_final])

    # è¼¸å‡ºæ ¼å¼çµ±ä¸€
    recommendations = recommendations.copy()
    recommendations['cluster'] = recommendations[cluster_col]
    recommendations = recommendations[['track_name', 'artists', 'cluster', 'distance']]

    logger.info("\nğŸµ Recommendations for '%s' (Artist: %s):", song_name, artist if artist else 'Any')
    # print(recommendations)

    return recommendations

# è‡ªå‹•å»ºç«‹ embedding æ¬„ä½åç¨±
embedding_cols = [f"emb_{i}" for i in range(X_emb.shape[1])]
# å¯«å…¥ df
df[embedding_cols] = X_emb
# recommend_song_ae_cluster(cluster_col="cluster_ae_kmeans_k8",embedding_cols=embedding_cols,song_name="Comedy",artist="Gen Hoshino")

for k in k_list:
    cluster_col = f"cluster_ae_kmeans_k{k}"
    logger.info("\n===== æ¨è–¦çµæœï¼ˆk = %dï¼‰=====", k)

    rec = recommend_song_ae_cluster(
        cluster_col=cluster_col,
        embedding_cols=embedding_cols,
        song_name="Comedy",
        artist="Gen Hoshino"
    )

    logger.info("\n%s", rec)
timer.log("Recommendation")

# ====== è¨˜éŒ„æ•´é«”çµæŸæ™‚é–“ ======
experiment_end = time.time()
total_elapsed = experiment_end - experiment_start

h = int(total_elapsed // 3600)
m = int((total_elapsed % 3600) // 60)
s = total_elapsed % 60

logger.info("===== Experiment finished =====")
logger.info("Total runtime: %d h %d m %.2f s", h, m, s)
